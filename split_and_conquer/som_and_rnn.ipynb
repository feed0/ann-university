{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Redes Neurais Artificiais` (RNA)\n",
    "\n",
    "### Trabalho Prático - Entrega 25/11/2024 até 23:55.\n",
    "\n",
    "### Dom Helder Escola Superior - 2024-2\n",
    "\n",
    "#### Prof. Dr. Marcos W. Rodrigues - marcos.rodrigues@academico.domhelder.edu.br\n",
    "\n",
    "---\n",
    "\n",
    "### Instruções para envio do trabalho:\n",
    "* Este trabalho prático deve ser realizado com no máximo 03 (três) integrantes.\n",
    "* Implementar as seguintes redes neurais: MLP, SOM, RNN, SVM e CNN.\n",
    "* Faça o download deste arquivo no formato `ipynb` em uma pasta, por exemplo, `Donwloads` ou `Meus Documentos`.\n",
    "* Utilize a linguagem de programação Python, e a interface de sua preferência, como VSCode, Microsoft Colab, PyCharm, Spyder, etc.\n",
    "* Salve/imprima este arquivo (.ipynb) no formato PDF (Microsoft Print to PDF), também na pasta `Donwloads` ou `Meus Documentos`, como preferir.\n",
    "* Compacte ambos os arquivos `ipynb` e `pdf` em um único arquivo `ZIP` ou `RAR`, e nomeie o aquivo como: `RNA_Aluno1_Aluno2_Aluno3.zip`.\n",
    "* Envie o arquivo `ZIP`, recém criado, no ambiente `Moodle`, acessando a guia `Avaliações Múltiplas`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"0\">RA/Aluno(a) 1: ________________________________________________</a>\n",
    "#### <a id=\"0\">RA/Aluno(a) 2: ________________________________________________</a>\n",
    "#### <a id=\"0\">RA/Aluno(a) 3: ________________________________________________</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bom Trabalho!\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 01 - Multi-Layer Perceptron\n",
    "\n",
    "    - MLP: Rede Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções:\n",
    "\n",
    "1. Acesse a documentação do Multi-Layer Perceptron via link: \n",
    "    * https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "2. Estude os seguintes parâmetros:\n",
    "    * 'activation' : Função de ativação para a camada oculta ({'identity', 'logistic', 'tanh', 'relu'}).\n",
    "    * 'solver' : O solucionador para otimização de peso {'lbfgs', 'sgd', 'adam'}. São métodos para a minimização de erros pela descida do gradiente.\n",
    "    * 'learning_rate_init' : A taxa de aprendizagem controla o tamanho do passo na atualização dos pesos. Usado apenas quando solver='sgd' ou 'adam'.\n",
    "    * 'max_iter' : Número máximo de iterações. O solucionador itera até a convergência (determinada por 'tol') ou este número de iterações (epochs).\n",
    "    * 'tol' : Tolerância para a otimização.\n",
    "    * 'momentum' : Momento para atualização de descida gradiente. Deve estar entre 0 e 1. Usado apenas quando solucionador='sgd'.\n",
    "    * 'Shuffle' : Se as amostras devem ser embaralhadas em cada iteração. Usado apenas quando solucionador='sgd' ou 'adam'.\n",
    "    * 'random_state' : Tornar reproduzível a geração de números aleatórios para inicialização de pesos e polarizações para treino e teste.\n",
    "\n",
    "### Questões que devem ser respondidas:\n",
    "\n",
    "- Q1. [20%] Visando melhorar a resposta (predição) da MLP, faça combinações sistêmicas entre os parâmetros da rede neural, reajustando o `activation`, `solver`, `learning_rate`, `max_iter`, `tol`, etc.\n",
    "- Q2. [20%] Ao realizar predições, crie um código/função mapeando a resposta numérica para a resposta de uma das três classes {0:Setosa, 1:Versicolour, 2:Virginica}.\n",
    "- Q3. [20%] Insira, pelo menos, `10` vetores com valores de entradas `distintos` para que a MLP realize predições dos novos valores, retornando a classe de cada vetor de entrada.\n",
    "- Q4. [20%] Separe o dataset IRIS, o qual contém 150 registros, em subconjuntos de treino e teste, usando as proporções 80~20 (80% de treino e 20% de teste) e 70~30, respectivamente.\n",
    "- Q5. [20%] Para cada subconjunto de treino e teste, explicite as métricas de avaliação como os valores de `precision`, `recall`, `f-measure`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Base de Dados\n",
    "\n",
    "Repositório UCI - https://archive.ics.uci.edu/\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/dataset/53/iris\n",
    "\n",
    "    Descrição das variáveis:\n",
    "\n",
    "    - Variáveis      Papel     Tipo         Values\n",
    "    - sepal length   Feature   Continuous\n",
    "    - sepal width    Feature   Continuous\n",
    "    - petal length   Feature   Continuous\n",
    "    - petal width    Feature   Continuous\n",
    "    - class\t         Target    Categorical  {Setosa; Versicolour; Virginica}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U scikit-learn\n",
    "\n",
    "# from sklearn import datasets\n",
    "# import pandas as pd\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# from sklearn.metrics import classification_report\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Definindo a base de dados (dataset) IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Atribuindo o dataset para as entradas e saídas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs  = iris.data\n",
    "print(\"Entradas: \", inputs.shape)\n",
    "outputs = iris.target\n",
    "print(\"Saídas: \", outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Criando a RNA Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier # Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Treinamento da MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Realizar previsões pela MLP treinada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código...\n",
    "\n",
    "# Ajuste o código para que a resposta seja uma das três classes {Setosa, Versicolour, Virginica}, e não apenas os números das classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Mapeamento de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para converter as classes dos iris\n",
    "\n",
    "# Código..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Divisão da base de dados: 80/20 e 70/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Dividindo o conjunto de dados em treino e teste com uma proporção de 80-20\n",
    "\n",
    "# Código...\n",
    "\n",
    "# Dividindo o conjunto de dados em treino e teste com uma proporção de 70-30\n",
    "\n",
    "# Código..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando o Grid-search com as bases de dados 80/20 e 70/30\n",
    "\n",
    "# Código..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 02 - SOM e RNN\n",
    "\n",
    "    - SOM: Mapa Auto-Organizável\n",
    "    - RNN: Rede Neural Recorrente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções:\n",
    "\n",
    "1. Deverá ser implementada as seguintes Redes Neurais Artificiais:\n",
    "    * `SOM` (_Self-Organizing Maps_), entre as bibliotecas `pymvpa` ou `pypi`, escolha apenas uma para opção implementar:\n",
    "        - http://www.pymvpa.org/examples/som.html\n",
    "        `ou`\n",
    "        - https://pypi.org/project/MiniSom/\n",
    "    * `RNN` (_Recurrent Neural Network_), entre as bibliotecas `tensorfow` ou `keras`, escolha apenas uma opção para implementar:\n",
    "        - Tensorflow: https://www.tensorflow.org/guide/keras/working_with_rnns\n",
    "        `ou`\n",
    "        - Keras: https://keras.io/api/layers/recurrent_layers/ (SimpleRNN layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Base de Dados\n",
    "\n",
    "- Utilize a base de dados do repositório Scikit-learn - https://scikit-learn.org/stable/datasets/\n",
    "- Selecione o datase de `Diabetes`: https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset\n",
    "    - Dataset Characteristics: _Multivariate_\n",
    "    - Associated Tasks: _Classification_, _Regression_\n",
    "    - Feature Type: _Real_\n",
    "    - #Instances: _442_\n",
    "    - #Features: _10_\n",
    "    - #Target: _1_\n",
    "\n",
    "    ##### Dataset de Diabetes:\n",
    "\n",
    "        - Cod  Col  Atributos                                       Papel     TipoVar       Valores\n",
    "        - age  age  age in years                                    Feature   Continuous\n",
    "        - sex  sex                                                  Feature   Continuous\n",
    "        - bmi  bmi  body mass index                                 Feature   Continuous\n",
    "        - map  bp  average blood pressure                           Feature   Continuous\n",
    "        - tc   s1  tc, total serum cholesterol                      Feature   Continuous\n",
    "        - ldl  s2  ldl, low-density lipoproteins                    Feature   Continuous\n",
    "        - hdl  s3  hdl, high-density lipoproteins                   Feature   Continuous\n",
    "        - tch  s4  tch, total cholesterol / HDL                     Feature   Continuous\n",
    "        - ltg  s5  ltg, possibly log of serum triglycerides level   Feature   Continuous\n",
    "        - glu  s6  glu, blood sugar level                           Feature   Continuous\n",
    "        - y     y  disease progression                              Target    Continuous    [25.0 ~ 346.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U scikit-learn\n",
    "\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Definindo o dataset DIABETES ou o dataset do tema do TCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "          0.01990749, -0.01764613],\n",
       "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "         -0.06833155, -0.09220405],\n",
       "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "          0.00286131, -0.02593034],\n",
       "        ...,\n",
       "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "         -0.04688253,  0.01549073],\n",
       "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "          0.04452873, -0.02593034],\n",
       "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "         -0.00422151,  0.00306441]]),\n",
       " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "        220.,  57.]),\n",
       " 'frame': None,\n",
       " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 442\\n\\n:Number of Attributes: First 10 columns are numeric predictive values\\n\\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n:Attribute Information:\\n    - age     age in years\\n    - sex\\n    - bmi     body mass index\\n    - bp      average blood pressure\\n    - s1      tc, total serum cholesterol\\n    - s2      ldl, low-density lipoproteins\\n    - s3      hdl, high-density lipoproteins\\n    - s4      tch, total cholesterol / HDL\\n    - s5      ltg, possibly log of serum triglycerides level\\n    - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n',\n",
       " 'feature_names': ['age',\n",
       "  'sex',\n",
       "  'bmi',\n",
       "  'bp',\n",
       "  's1',\n",
       "  's2',\n",
       "  's3',\n",
       "  's4',\n",
       "  's5',\n",
       "  's6'],\n",
       " 'data_filename': 'diabetes_data_raw.csv.gz',\n",
       " 'target_filename': 'diabetes_target.csv.gz',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "df = datasets.load_diabetes()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Atribuindo o dataset para as entradas e saídas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atributos:  ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
      "Entradas:  (442, 10)\n",
      "Saídas:  (442,)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"Atributos: \", df.feature_names)\n",
    "# inputs  = MinMaxScaler().fit_transform(df.data)\n",
    "inputs  = df.data\n",
    "print(\"Entradas: \", inputs.shape)\n",
    "outputs = df.target\n",
    "print(\"Saídas: \", outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    SOM (_Self-Organizing Maps_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mvpa2.suite import *\n",
    "# pip install minisom\n",
    "from minisom import MiniSom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'learning_rate': 0.5, 'sigma': 2.0, 'x': 5, 'y': 5}\n",
      "Melhor pontuação (MSE): 3591.797928889407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "class MiniSomRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, x=10, y=10, input_len=10, sigma=1.0, learning_rate=0.5):\n",
    "        # Inicializa os hiperparâmetros do SOM\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.input_len = input_len\n",
    "        self.sigma = sigma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.som = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Cria o MiniSom e treina\n",
    "        self.som = MiniSom(self.x, self.y, self.input_len, sigma=self.sigma, learning_rate=self.learning_rate)\n",
    "        self.som.train(X, 500, random_order=True)\n",
    "        \n",
    "        # Associar a variável target (y) aos neurônios\n",
    "        self.y_som = np.zeros((self.x, self.y))\n",
    "        self.neuron_count = np.zeros((self.x, self.y))\n",
    "        for i in range(len(X)):\n",
    "            winner = self.som.winner(X[i])\n",
    "            self.y_som[winner[0], winner[1]] += y[i]\n",
    "            self.neuron_count[winner[0], winner[1]] += 1\n",
    "        \n",
    "        self.y_som = np.divide(self.y_som, self.neuron_count, where=self.neuron_count != 0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Previsão para novos dados\n",
    "        predictions = np.array([self._predict_single(x) for x in X])\n",
    "        return predictions\n",
    "\n",
    "    def _predict_single(self, x):\n",
    "        # Prever para uma única entrada\n",
    "        winner = self.som.winner(x)\n",
    "        return self.y_som[winner[0], winner[1]]\n",
    "\n",
    "\n",
    "# Carregar o dataset de diabetes\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data  # Características\n",
    "y = diabetes.target  # Target (estágio da diabetes)\n",
    "\n",
    "# Escalonamento dos dados (é importante escalar antes de treinar o SOM)\n",
    "# X_scaled = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Definir o modelo\n",
    "# som_model = MiniSomRegressor(input_len=X_scaled.shape[1])\n",
    "som_model = MiniSomRegressor(input_len=X.shape[1])\n",
    "\n",
    "# Definir o espaço de parâmetros para busca\n",
    "param_grid = {\n",
    "    'x': [5, 10, 12, 16, 20],  # Tamanho da grade (X)\n",
    "    'y': [5, 10, 12, 16, 20],  # Tamanho da grade (Y)\n",
    "    'sigma': [0.5, 1.0, 2.0, 3.0, 4.0, 5.0],  # Valor de sigma\n",
    "    'learning_rate': [0.1, 0.5, 0.9, 2, 3],  # Taxa de aprendizado\n",
    "}\n",
    "\n",
    "# Configurar o GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=som_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Realizar a busca em grade\n",
    "# grid_search.fit(X_scaled, y)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Exibir os melhores parâmetros encontrados\n",
    "print(\"Melhores parâmetros:\", grid_search.best_params_)\n",
    "print(\"Melhor pontuação (MSE):\", -grid_search.best_score_)  # A pontuação é negativa devido ao 'neg_mean_squared_error'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "malha_neuronios = (5, 5)\n",
    "som = MiniSom(malha_neuronios[0], malha_neuronios[1], 10, learning_rate=0.5, sigma=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.25, random_state=101)\n",
    "som.train(X_train, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[102.69230769,  79.2       ,  63.1       ,  99.21428571,\n",
       "         83.73333333],\n",
       "       [125.45      , 130.2       , 125.07142857, 133.28571429,\n",
       "        134.54545455],\n",
       "       [170.61904762, 117.92307692, 130.61111111, 199.71428571,\n",
       "        167.2       ],\n",
       "       [150.71428571, 172.69230769, 179.27272727, 219.42857143,\n",
       "        214.95      ],\n",
       "       [186.06666667, 135.53846154, 220.57142857, 258.8       ,\n",
       "        248.4       ]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_som = np.zeros(malha_neuronios)\n",
    "neuron_count = np.zeros(malha_neuronios)\n",
    "\n",
    "# Percorrendo todos os pontos de entrada e associando os valores de y ao neurônio correspondente\n",
    "# Normalizando a matriz de previsões (somando as saídas por neurônio)\n",
    "for i in range(len(X_train)):\n",
    "    winning_node = som.winner(X_train[i])\n",
    "    y_som[winning_node] += y_train[i]\n",
    "    neuron_count[winning_node] += 1\n",
    "\n",
    "y_som = np.divide(y_som, neuron_count, where=neuron_count != 0)\n",
    "y_som"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = 0\n",
    "# win = som.winner(X_test[pos])\n",
    "# print(win)\n",
    "# print(\"real:\", y_test[pos])\n",
    "# print(\"prediction:\", y_som[win])\n",
    "\n",
    "def predict_som(som, X_new):\n",
    "    winning_node = som.winner(X_new)\n",
    "    return y_som[winning_node[0], winning_node[1]]\n",
    "\n",
    "y_pred = np.array([predict_som(som, X_test[i]) for i in range(len(X_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error:  3214.7989251069025\n",
      "error:  56.69919686474318\n",
      "r2_score: 0.4644411321010765\n"
     ]
    }
   ],
   "source": [
    "from numpy import sqrt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "squared_error = mean_squared_error(y_test, y_pred)\n",
    "print(\"mean_squared_error: \", squared_error)\n",
    "print(\"error: \", sqrt(squared_error))\n",
    "print(\"r2_score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    RNN (_Recurrent Neural Network_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "data = load_diabetes()\n",
    "X = data.data  # Atributos\n",
    "y = data.target  # Alvo\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshaping dos dados para a entrada da RNN. Formato (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[MeanSquaredError(), MeanAbsoluteError()])\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "loss, mse, mae = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Mean Absolute Error: {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 03 - SVM (SVC e SVR) e CNN\n",
    "\n",
    "    - SVM: Máquina de Vetor de Suporte\n",
    "    - CNN: Rede Neural Convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções:\n",
    "\n",
    "1. Deverá ser implementada as seguintes Redes Neurais Artificiais:\n",
    "    * `SVM Classificador` (_Support Vector Machine : Regression_), utilizando a biblioteca `sklearn`, svm.SVC:\n",
    "        - https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "    * `SVM Regressor` (_Support Vector Machine : Classification_), utilizando a biblioteca `sklearn`, svm.SVR:\n",
    "        - https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\n",
    "    * `CNN` (_Convolutional Neural Networks_), utilizando a biblioteca `tensorflow` com a base de dados `CIFAR10` utilizada no exemplo do link:\n",
    "        - https://www.tensorflow.org/tutorials/images/cnn?hl=pt-br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Base de Dados\n",
    "\n",
    "- Utilize a base de dados do repositório Scikit-learn - https://scikit-learn.org/stable/datasets/\n",
    "- Selecione o datase de `Diabetes`: https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset\n",
    "    - Dataset Characteristics: _Multivariate_\n",
    "    - Associated Tasks: _Classification_, _Regression_\n",
    "    - Feature Type: _Real_\n",
    "    - #Instances: _442_\n",
    "    - #Features: _10_\n",
    "    - #Target: _1_\n",
    "\n",
    "    ##### Dataset de Diabetes:\n",
    "\n",
    "        - Cod  Col  Atributos                                       Papel     TipoVar       Valores\n",
    "        - age  age  age in years                                    Feature   Continuous\n",
    "        - sex  sex                                                  Feature   Continuous\n",
    "        - bmi  bmi  body mass index                                 Feature   Continuous\n",
    "        - map  bp  average blood pressure                           Feature   Continuous\n",
    "        - tc   s1  tc, total serum cholesterol                      Feature   Continuous\n",
    "        - ldl  s2  ldl, low-density lipoproteins                    Feature   Continuous\n",
    "        - hdl  s3  hdl, high-density lipoproteins                   Feature   Continuous\n",
    "        - tch  s4  tch, total cholesterol / HDL                     Feature   Continuous\n",
    "        - ltg  s5  ltg, possibly log of serum triglycerides level   Feature   Continuous\n",
    "        - glu  s6  glu, blood sugar level                           Feature   Continuous\n",
    "        - y     y  disease progression                              Target    Continuous    [25.0 ~ 346.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U scikit-learn\n",
    "\n",
    "# from sklearn import datasets\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# from sklearn import svm\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Definindo o dataset DIABETES ou o dataset do tema do TCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "          0.01990749, -0.01764613],\n",
       "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "         -0.06833155, -0.09220405],\n",
       "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "          0.00286131, -0.02593034],\n",
       "        ...,\n",
       "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "         -0.04688253,  0.01549073],\n",
       "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "          0.04452873, -0.02593034],\n",
       "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "         -0.00422151,  0.00306441]]),\n",
       " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "        220.,  57.]),\n",
       " 'frame': None,\n",
       " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 442\\n\\n:Number of Attributes: First 10 columns are numeric predictive values\\n\\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n:Attribute Information:\\n    - age     age in years\\n    - sex\\n    - bmi     body mass index\\n    - bp      average blood pressure\\n    - s1      tc, total serum cholesterol\\n    - s2      ldl, low-density lipoproteins\\n    - s3      hdl, high-density lipoproteins\\n    - s4      tch, total cholesterol / HDL\\n    - s5      ltg, possibly log of serum triglycerides level\\n    - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n',\n",
       " 'feature_names': ['age',\n",
       "  'sex',\n",
       "  'bmi',\n",
       "  'bp',\n",
       "  's1',\n",
       "  's2',\n",
       "  's3',\n",
       "  's4',\n",
       "  's5',\n",
       "  's6'],\n",
       " 'data_filename': 'diabetes_data_raw.csv.gz',\n",
       " 'target_filename': 'diabetes_target.csv.gz',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "df = datasets.load_diabetes()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Atribuindo o dataset para as entradas e saídas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Atributos: \", df.feature_names)\n",
    "inputs  = df.data\n",
    "print(\"Entradas: \", inputs.shape)\n",
    "outputs = df.target\n",
    "print(\"Saídas: \", outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    SVM Classificador (_SVC_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import Rbf\n",
    "from idaes.surrogates.pysmo import radial_basis_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Códigos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    SVM Regressor (_SVR_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvpa2.suite import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    CNN (_Convolutional Neural Network_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize a base de dados `CIFAR10` do link: https://www.tensorflow.org/tutorials/images/cnn?hl=pt-br\n",
    "\n",
    "# Carregamento da base de dados..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fim do TP! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
